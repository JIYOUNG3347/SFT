# -*- coding: utf-8 -*-
"""SFT_CoT1_FFT.ipynb

Automatically generated by Colab.
"""

!pip install -q datasets transformers accelerate trl peft bitsandbytes

from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments
from trl import SFTTrainer, SFTConfig
import torch

# 1. ë°ì´í„°ì…‹ ë¡œë”©
dataset = load_dataset("openai/gsm8k", "main", split="train[:2000]")

# 2. ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
model_name = "Qwen/Qwen1.5-1.8B"  # ì˜ˆ: "meta-llama/Llama-2-7b-hf" ë˜ëŠ” "google/gemma-2b"
# model_name = "facebook/opt-350m"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# 3. í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜
def make_prompt(example, with_prompt = True):
  question = example["question"].strip()
  if with_prompt:
    return f"{question}\nLet's think step by step.\n"
  else:
    return question

# 4. ìƒì„± í•¨ìˆ˜(í•™ìŠµ ì „í›„ í…ŒìŠ¤íŠ¸)
def predict_sentiment(example, model, tokenizer, with_prompt=True, max_new_tokens=100):
    prompt = make_prompt(example, with_prompt=with_prompt)  # dict ê¸°ë°˜
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512).to(model.device)

    with torch.no_grad():
        outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)

    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return decoded

# 5. í•™ìŠµ ì „ í…ŒìŠ¤íŠ¸ ê¸°ë¡
sample = dataset[20]
before_no_prompt = predict_sentiment(sample, model, tokenizer, with_prompt=False)
before_with_prompt = predict_sentiment(sample, model, tokenizer, with_prompt=True)

# 6. ë°ì´í„° ì „ì²˜ë¦¬: prompt + ì •ë‹µìœ¼ë¡œ text êµ¬ì„±
def preprocess(example):
  prompt = make_prompt(example, with_prompt=True)
  answer = example["answer"].strip()
  return {"text": prompt + answer + tokenizer.eos_token}

train_dataset = dataset.map(preprocess)

print("Raw Dataset: \n",sample, "\n")
print("Dataset with prompt: \n", make_prompt(sample), "\n")
print("Dataset no prompt: \n", make_prompt(sample, with_prompt=False), "\n")
print("Preprocess Datset: \n", preprocess(sample), "\n")

# 7. SFT ì„¤ì •
training_args = SFTConfig(
    output_dir="./sft-gemma-cot",
    per_device_train_batch_size=1,
    num_train_epochs=2,
    max_seq_length=512,
    logging_steps=10,
    save_steps=100,
    save_total_limit=1,
)

# 8. SFTTrainerë¡œ í•™ìŠµ
trainer = SFTTrainer(
    model=model,
    train_dataset=train_dataset,
    args=training_args,
)
trainer.train()

# 9. í•™ìŠµ í›„ í…ŒìŠ¤íŠ¸ ê¸°ë¡
after_no_prompt = predict_sentiment(sample, model, tokenizer, with_prompt=False)
after_with_prompt = predict_sentiment(sample, model, tokenizer, with_prompt=True)

# 10. í•™ìŠµ ì „í›„ í…ŒìŠ¤íŠ¸ ë¹„êµ
print("ğŸŸ¡ BEFORE fine-tuning")
print(f"[No prompt]\n{before_no_prompt}")
print(f"[With prompt]\n{before_with_prompt}")

print("\nğŸŸ¢ AFTER fine-tuning")
print(f"[No prompt]\n{after_no_prompt}")
print(f"[With prompt]\n{after_with_prompt}")

