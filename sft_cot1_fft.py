# -*- coding: utf-8 -*-
"""SFT_CoT1_FFT.ipynb

Automatically generated by Colab.
"""

!pip install -q datasets transformers accelerate trl peft bitsandbytes

from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments
from trl import SFTTrainer, SFTConfig
import torch

# 1. 데이터셋 로딩
dataset = load_dataset("openai/gsm8k", "main", split="train[:2000]")

# 2. 모델과 토크나이저 로드
model_name = "Qwen/Qwen1.5-1.8B"  # 예: "meta-llama/Llama-2-7b-hf" 또는 "google/gemma-2b"
# model_name = "facebook/opt-350m"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# 3. 프롬프트 생성 함수
def make_prompt(example, with_prompt = True):
  question = example["question"].strip()
  if with_prompt:
    return f"{question}\nLet's think step by step.\n"
  else:
    return question

# 4. 생성 함수(학습 전후 테스트)
def predict_sentiment(example, model, tokenizer, with_prompt=True, max_new_tokens=100):
    prompt = make_prompt(example, with_prompt=with_prompt)  # dict 기반
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512).to(model.device)

    with torch.no_grad():
        outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)

    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return decoded

# 5. 학습 전 테스트 기록
sample = dataset[20]
before_no_prompt = predict_sentiment(sample, model, tokenizer, with_prompt=False)
before_with_prompt = predict_sentiment(sample, model, tokenizer, with_prompt=True)

# 6. 데이터 전처리: prompt + 정답으로 text 구성
def preprocess(example):
  prompt = make_prompt(example, with_prompt=True)
  answer = example["answer"].strip()
  return {"text": prompt + answer + tokenizer.eos_token}

train_dataset = dataset.map(preprocess)

print("Raw Dataset: \n",sample, "\n")
print("Dataset with prompt: \n", make_prompt(sample), "\n")
print("Dataset no prompt: \n", make_prompt(sample, with_prompt=False), "\n")
print("Preprocess Datset: \n", preprocess(sample), "\n")

# 7. SFT 설정
training_args = SFTConfig(
    output_dir="./sft-gemma-cot",
    per_device_train_batch_size=1,
    num_train_epochs=2,
    max_seq_length=512,
    logging_steps=10,
    save_steps=100,
    save_total_limit=1,
)

# 8. SFTTrainer로 학습
trainer = SFTTrainer(
    model=model,
    train_dataset=train_dataset,
    args=training_args,
)
trainer.train()

# 9. 학습 후 테스트 기록
after_no_prompt = predict_sentiment(sample, model, tokenizer, with_prompt=False)
after_with_prompt = predict_sentiment(sample, model, tokenizer, with_prompt=True)

# 10. 학습 전후 테스트 비교
print("🟡 BEFORE fine-tuning")
print(f"[No prompt]\n{before_no_prompt}")
print(f"[With prompt]\n{before_with_prompt}")

print("\n🟢 AFTER fine-tuning")
print(f"[No prompt]\n{after_no_prompt}")
print(f"[With prompt]\n{after_with_prompt}")

